# 数据清洗

我们来详细梳理一下**数据清洗 (Data Cleaning)** 这个极其重要，但又常常被忽视的环节。

在任何数据分析或建模项目中，数据清洗都占据了**60%-80%** 的时间。它就像是**“为大餐备菜”**的过程——如果你的食材（数据）不新鲜、不干净、处理得乱七八糟，那么无论你后面的厨师（算法）多么高明，也做不出一道美味佳肴。

一个高质量的数据清洗流程，能为后续的分析和建模奠定坚实的基础。

---

### 一、 数据清洗的核心目标

数据清洗的目标是识别并处理原始数据中的**“脏”数据**，最终得到一个**干净、准确、一致、完整**的数据集。所谓的“脏”数据，主要包括：

*   **缺失值 (Missing Values)**
*   **异常值/离群点 (Outliers)**
*   **重复值 (Duplicate Data)**
*   **不一致的数据 (Inconsistent Data)**
*   **无效/错误的数据 (Invalid Data)**

---

### 二、 数据清洗的一般流程（一个SOP）

一个标准的数据清洗流程可以分为以下几个步骤：

#### **Step 0: 理解数据 (Understand Your Data)**

这是所有工作开始之前最重要的一步！
*   **获取数据字典**：了解每个字段（列）的**含义**、**数据类型**（是数值、文本还是日期？）、**单位**以及**合理的取值范围**。
*   **初步探索**：
    *   使用 `.info()` 查看每列的数据类型和非空值数量，快速发现缺失情况。
    *   使用 `.describe()` 查看数值型数据的描述性统计（均值、标准差、最大/最小值等），快速发现异常值。
    *   使用 `.value_counts()` 查看类别型数据的取值分布，发现不一致的拼写或错误条目。

#### **Step 1: 处理缺失值 (Handling Missing Values)**

这是最常见的数据质量问题。

*   **识别**：使用 `isnull().sum()` 快速统计每列的缺失值数量。
*   **处理策略**：
    1.  **删除 (Deletion)**：
        *   **删除行**：如果某一行的大部分关键特征都缺失，或者缺失值的比例非常小（比如<1%），可以直接删除这些行。
        *   **删除列**：如果某一列的缺失值比例非常高（比如>50%），且这个特征又不是至关重要的，可以考虑删除整列。
    2.  **填充 (Imputation)**：这是更常用的方法。
        *   **简单填充**：
            *   **均值/中位数**：对于数值型数据，用该列的均值（数据分布对称时）或中位数（数据分布倾斜或有异常值时）来填充。
            *   **众数**：对于类别型数据，用出现次数最多的类别来填充。
            *   **固定值**：用一个特定的值（如0, -1, "Unknown"）来填充，让模型知道这是一个填充值。
        *   **高级填充**：
            *   **插值法**：对于时间序列数据，使用前一个值、后一个值或线性插值来填充，更符合时间规律。
            *   **模型预测填充**：将含有缺失值的列作为**目标变量**，用其他所有列作为特征，训练一个预测模型（如线性回归、K-NN、随机森林）来**预测**这些缺失值。这是最复杂但通常效果最好的方法。

#### **Step 2: 处理异常值 (Handling Outliers)**

异常值是那些与数据集中其他观测值显著不同的数据点。

*   **识别**：
    *   **可视化**：绘制**箱线图 (Box Plot)** 或**散点图**，能直观地发现离群点。
    *   **统计方法**：
        *   **3σ法则 (Z-score)**：对于近似正态分布的数据，可以认为偏离均值超过3个标准差的数据点是异常值。
        *   **IQR法则 (Interquartile Range)**：`Q1 - 1.5*IQR` 到 `Q3 + 1.5*IQR` 之外的点被视为异常值。这是箱线图的判断依据，对非正态分布的数据更稳健。
*   **处理策略**：
    1.  **删除**：如果确定是**录入错误**或**测量错误**，且数量不多，可以直接删除。
    2.  **转换 (Transformation)**：对数据进行**对数转换 (log transform)** 或**平方根转换**，可以“拉近”异常值与普通值的距离，减小其影响力。
    3.  **盖帽/缩尾 (Capping/Winsorizing)**：将所有超出某个阈值（如99%分位数）的异常值，都用该阈值替换。
    4.  **视为缺失值处理**：先将异常值标记为缺失，然后用Step 1中的填充方法来处理。

#### **Step 3: 处理重复值 (Handling Duplicates)**

*   **识别**：使用 `duplicated().sum()` 检查完全重复的行。
*   **处理策略**：通常情况下，完全重复的行是由于数据采集或合并错误造成的，可以直接使用 `drop_duplicates()` **删除**。在删除前，需要确认这些重复是否是业务上有意义的（虽然这种情况很少）。

#### **Step 4: 处理不一致/无效数据 (Handling Inconsistent/Invalid Data)**

这类问题需要更多的“侦探工作”和对业务的理解。

*   **不一致性 (Inconsistency)**：
    *   **格式不一致**：
        *   **日期**：`"2023/01/05"`, `"05-Jan-2023"`, `"20230105"` -> 统一为标准日期时间格式。
        *   **单位**：`"100cm"`, `"1m"` -> 统一单位。
    *   **文本不一致**：
        *   **大小写**：`"USA"`, `"usa"`, `"U.S.A"` -> 统一为 `"USA"`。
        *   **拼写错误/别名**：`"New York"`, `"NY"`, `"NewYork"` -> 统一为标准名称。
        *   **多余的空格**：`" apple "`, `"apple"` -> 使用 `.strip()` 去除首尾空格。
*   **无效性 (Invalidity)**：
    *   **超出合理范围**：
        *   **年龄**：-5岁 或 200岁。
        *   **性别**：除了“男”、“女”之外的奇怪字符。
        *   **打分**：在一个1-5分的评价体系中出现6分。
    *   **逻辑矛盾**：
        *   “注册日期”晚于“最后登录日期”。
        *   “未成年人”的“婚姻状况”为“已婚”。
*   **处理策略**：
    *   **标准化/替换**：通过编写规则、正则表达式或使用查找替换表，将不一致的数据统一。
    *   **纠正或删除**：对于无效数据，如果能追溯到源头进行纠正最好，否则通常将其视为**缺失值**处理或直接**删除**。

---

### Python Pandas中的实用函数总结

| 清洗任务       | **Pandas 常用函数**                                          |
| :------------- | :----------------------------------------------------------- |
| **理解数据**   | `.info()`, `.describe()`, `.value_counts()`, `.shape`        |
| **处理缺失值** | `.isnull()`, `.notnull()`, `.dropna()`, `.fillna()`          |
| **处理异常值** | `.quantile()`, `.clip()`, `np.log1p()`                       |
| **处理重复值** | `.duplicated()`, `.drop_duplicates()`                        |
| **处理不一致** | `.str.lower()`, `.str.strip()`, `.replace()`, `pd.to_datetime()` |

数据清洗是一个迭代的过程，你可能需要反复地进行检查和处理，直到你对数据的质量感到满意为止。这是一个细致活，但其回报是巨大的——**高质量的数据是高质量分析和建模的唯一保障**。